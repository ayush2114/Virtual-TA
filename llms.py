import requests
from dotenv import load_dotenv
import os
import json
from typing import List, Dict, Any, Optional, Union
import base64

prompt_template = f"""
You are an assistant for the 'Tools in Data Science' course. Given a question and context, respond with:

- A concise, accurate answer in plain English (no markdown or formatting).
- in JSON format.
"""

tools = [{
    "type": "function",
    "function": {
        "name": "generate_structured_response",
        "description": "Generate a structured response with an answer and URLs of the contexts used",
        "parameters": {
            "type": "object",
            "properties": {
                "answer": {
                    "type": "string",
                    "description": "The answer to the user's question"
                },
                "links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string", 
                                "description": "URL of the context used"
                            },
                            "text": {
                                "type": "string",
                                "description": "Very brief justification of why this source is relevant to the answer"
                            }
                        }
                    },
                    "description": "Relevant sources that support the answer"
                }
            },
            "required": ["answer"]
        }
    }
}]

load_dotenv()

class OpenRouter:
    def __init__(self, model: str = "mistralai/mistral-small-3.1-24b-instruct:free"):
        self.model = model
        self.url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {os.environ.get('OPENROUTER_API_KEY')}",
            "Content-Type": "application/json"
        }

    def get_image_description(self, image: str) -> str:
        """
        Get a description of the image using the OpenRouter API.

        Args:
            image (str): Base64-encoded image string.

        Returns:
            str: The description of the image.
        """
        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": [{"type": "text", "text": "Please provide a detailed description of what this image contains."},{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image}"}}]}
            ]
        }
        response = requests.post(self.url, headers=self.headers, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    
    def generate_answer(self, question: str, context: str) -> str:
        """
        Generate an answer to a question using the OpenRouter API.

        Args:
            question (str): The question to answer.
            context (str): Context to help answer the question.

        Returns:
            str: The generated answer.
        """
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": prompt_template},
                {"role": "user", "content": [{"type": "text", "text": question}]},
                {"role": "assistant", "content": [{"type": "text", "text": context}]}
            ],
            "tools": tools,
            "tool_choice": "required"
        }
        response = requests.post(self.url, headers=self.headers, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['tool_calls'][0]['function']['arguments']

class Nomic:
    def __init__(self, model: str = "nomic-embed-text-v1.5"):
        self.model = model
        self.url = "https://api-atlas.nomic.ai/v1/embedding/text"  # Removed comma here
        self.headers = {
            "Authorization": f"Bearer {os.environ.get('NOMIC_API_KEY')}",
            "Content-Type": "application/json"
        }

    def get_embedding(self, text: str) -> list:
        """
        Gets the embedding for a given text using the Nomic API.

        Args:
            text (str): The text to embed.

        Returns:
            list: The embedding vector.
        """
        payload = {
            "model": self.model,
            "task_type": "search_document",
            "texts": [text]
        }
        response = requests.post(self.url, headers=self.headers, json=payload)
        print(f"[DEBUG] Nomic API response status code: {response.status_code}")
        return response.json()['embeddings'][0]
    
    def get_batch_embedding(self, texts: list) -> list:
        """
        Gets embeddings for a batch of texts using the Nomic API.

        Args:
            texts (list): List of texts to embed.

        Returns:
            list: List of embedding vectors.
        """
        payload = {
            "model": self.model,
            "task_type": "search_document",
            "texts": texts
        }
        response = requests.post(self.url, headers=self.headers, json=payload)
        return response.json()['embeddings']

class OpenAI:
    def __init__(self, model: str = "gpt-4o-mini"):
        self.model = model
        self.url = "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {os.environ.get('AIPROXY_TOKEN')}",
            "Content-Type": "application/json"
        }
    
    def generate_response(self, question: str, context: List[Dict[str, str]] = None, 
                         image_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Generate a structured response with answer and links using OpenAI's function calling capabilities.
        
        Args:
            question (str): User's question
            context (List[Dict]): List of context documents with 'text' and 'url' fields
            image_path (str, optional): Path to an image file if provided
            
        Returns:
            Dict: Structured response with answer and links
        """
        print(f"[DEBUG] generate_response: Starting with question: '{question[:50]}...'")
        print(f"[DEBUG] generate_response: Context provided: {len(context) if context else 0} items")
        print(f"[DEBUG] generate_response: Image path: {image_path}")
        
        # Create system message
        system_message = "You are a helpful teaching assistant. Answer questions based on the context provided."
        
        # Add image description to system message if an image is provided
        if image_path:
            try:
                print(f"[DEBUG] generate_response: Getting image description for: {image_path}")
                image_description = self.get_image_description(image_path)
                print(f"[DEBUG] generate_response: Got image description, length: {len(image_description)}")
                system_message += "\n\nThe user has provided an image. Here is a detailed description of that image:\n\n"
                system_message += image_description
                system_message += "\n\nUse this image description along with any other context to answer the user's question."
            except Exception as e:
                print(f"[DEBUG] generate_response: Error getting image description: {str(e)}")
                print(f"[DEBUG] generate_response: Error type: {type(e).__name__}")
        
        print(f"[DEBUG] generate_response: Final system message length: {len(system_message)}")
        messages = [{"role": "system", "content": system_message}]
        
        # Add context message if available
        if context:
            context_str = "\n\n".join([f"Source ({doc['url']}):\n{doc['text']}" for doc in context])
            messages.append({"role": "system", "content": f"Use the following context to answer the question:\n\n{context_str}"})
        
        # Create the user message with optional image
        user_message = {"role": "user", "content": []}
        
        # Add text content
        user_message["content"].append({
            "type": "text", 
            "text": question
        })
        
        # Add image content if provided
        if image_path:
            image_b64 = self._encode_image(image_path)
            user_message["content"].append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{image_b64}"
                }
            })
        
        messages.append(user_message)
        
        # Define the function for structured output
        functions = [{
            "type": "function",
            "function": {
                "name": "generate_structured_response",
                "description": "Generate a structured response with an answer and relevant links",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "answer": {
                            "type": "string",
                            "description": "The answer to the user's question"
                        },
                        "links": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "url": {
                                        "type": "string", 
                                        "description": "URL of the source"
                                    },
                                    "text": {
                                        "type": "string",
                                        "description": "Brief description of what the source contains"
                                    }
                                }
                            },
                            "description": "Relevant sources that support the answer"
                        }
                    },
                    "required": ["answer"]
                }
            }
        }]
        
        # Prepare payload for API call
        payload = {
            "model": self.model,
            "messages": messages,
            "functions": functions,            "function_call": {"name": "generate_structured_response"},
            "temperature": 0.3,
            "max_tokens": 1000
        }
        response = requests.post(self.url, headers=self.headers, json=payload)
        response.raise_for_status()
        
        # Extract function call response
        function_call = response.json()["choices"][0]["message"]["function_call"]
        structured_response = json.loads(function_call["arguments"])
        return structured_response
    
    def get_image_description(self, image_path: str) -> str:
        """
        Get a detailed description of an image that can be used by an LLM to answer user questions.
        
        Args:
            image_path (str): Path to the image file
            
        Returns:
            str: Detailed description of the image
        """
        print(f"[DEBUG] get_image_description: Starting with image_path: {image_path}")
        try:
            # Encode the image to base64
            print("[DEBUG] get_image_description: Encoding image to base64")
            image_b64 = self._encode_image(image_path)
            print(f"[DEBUG] get_image_description: Successfully encoded image, length: {len(image_b64)}")
            
            messages = [
                {
                    "role": "system", 
                    "content": [
                        {
                            "type": "text", 
                            "text": (
                                "You are a vision assistant specialized in providing detailed descriptions of images "
                                "for educational purposes. Analyze the image thoroughly and provide a comprehensive "
                                "description covering:\n\n"
                                "1. Primary subject/topic - What is the main focus of the image?\n"
                                "2. Visual elements - Charts, graphs, diagrams, illustrations, screenshots, or photographs?\n"
                                "3. Text content - Transcribe any visible text, formulas, code, or labels\n"
                                "4. Data representation - Describe values, trends, and patterns in charts/graphs\n"
                                "5. Educational context - How does this relate to teaching/learning?\n"
                                "6. Technical details - For code/diagrams, explain technical elements\n\n"
                                "Structure your response to be directly usable by an LLM to answer student questions about the image. "
                                "Be specific, factual, and comprehensive."
                            )
                        }
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "Please provide a detailed description of this image."
                        },                    {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_b64}"
                            }
                        }
                    ]
                }
            ]
            
            payload = {
                "model": "gpt-4o-mini",  # Using GPT-4o-mini for best vision capabilities
                "messages": messages,
                "max_tokens": 1500
            }
            
            print("[DEBUG] get_image_description: Preparing to send request to OpenAI API")
            print(f"[DEBUG] get_image_description: API key present: {'Yes' if os.environ.get('OPENAI_API_KEY') else 'No'}")
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",  # Use direct OpenAI API for vision
                headers={
                    "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
                    "Content-Type": "application/json"
                },
                json=payload
            )
            print(f"[DEBUG] get_image_description: Response status code: {response.status_code}")
            response.raise_for_status()
            
            result = response.json()['choices'][0]['message']['content']
            print(f"[DEBUG] get_image_description: Successfully got image description, length: {len(result)}")
            return result
        except Exception as e:
            print(f"[DEBUG] get_image_description: Error: {str(e)}")
            print(f"[DEBUG] get_image_description: Error type: {type(e).__name__}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"[DEBUG] get_image_description: Error response: {e.response.text}")
            raise

class Ollama:
    def __init__(self, model_name: str = "gemma3:1b"):
        self.model_name = model_name

    def chat(self, messages: list[dict]) -> dict:
        url = f"http://localhost:11434/api/chat"
        payload = {
            "model": self.model_name,
            "messages": messages,
            "stream": False,
        }
        headers = {
            "Content-Type": "application/json",
        }
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code != 200:
            raise Exception(f"Error: {response.status_code} - {response.text}")
        
        return response.json()
    
    def get_image_description(self, image: str) -> str:
        """
        Get a description of the image using the Ollama API.

        Args:
            image (str): Base64-encoded image string.

        Returns:
            str: The description of the image.
        """
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "user", "content": [{"type": "text", "text": "Please provide a detailed description of what this image contains."},{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image}"}}]}
            ]
        }
        response = requests.post("http://localhost:11434/api/chat", json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    
    def generate_answer(self, question: str, context: str) -> str:
        """
        Generate an answer to a question using the Ollama API.

        Args:
            question (str): The question to answer.
            context (str): Context to help answer the question.

        Returns:
            str: The generated answer.
        """
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": "You are a teaching assistant for the course 'Tools in Data Science'. Answer the user's questions strictly based on the provided context. Do not provide explanationsâ€”only give direct, precise answers. Do not include contents of context in your response."},
                {"role": "user", "content": [{"type": "text", "text": question}]},
                {"role": "assistant", "content": [{"type": "text", "text": context}]}
            ]
        }
        response = requests.post("http://localhost:11434/api/chat", json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']